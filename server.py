# server.py
from fastapi import FastAPI, Request, BackgroundTasks
from crawlers.gmarket import crawl_gmarket
from crawlers.elevenst import crawl_elevenst
import uvicorn
import os
import httpx
from datetime import datetime

# âœ… ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ .env ë¡œë“œ
if os.getenv("ENV") != "production":
    from dotenv import load_dotenv
    load_dotenv()

STRAPI_API_URL = os.getenv("STRAPI_API_URL")

app = FastAPI()


@app.get("/")
async def root():
    return {"message": "Python FastAPI ì„œë²„ ì •ìƒ ë™ì‘ ì¤‘"}


# âœ… ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ Strapi ë°ì´í„° ì •ë¦¬ + ì €ì¥
async def save_to_strapi(results):
    if not results or not STRAPI_API_URL:
        return

    today = datetime.now().strftime("%Y-%m-%d")
    site = results[0].get("site")

    async with httpx.AsyncClient() as client:
        try:
            # 1. ì˜¤ëŠ˜ ë‚ ì§œ + ì‚¬ì´íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ
            query = f"filters[date][$eq]={today}&filters[site][$eq]={site}"
            check_res = await client.get(
                f"{STRAPI_API_URL}/crawls?{query}",
                headers={"Content-Type": "application/json"},
                timeout=30.0,
            )
            check_data = check_res.json().get("data", [])

            # 2. ê¸°ì¡´ ë°ì´í„° ìˆìœ¼ë©´ ì „ë¶€ ì‚­ì œ
            if check_data:
                print("ğŸ—‘ ì˜¤ëŠ˜ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ")
                for item in check_data:
                    item_id = item.get("id")
                    if item_id:
                        await client.delete(
                            f"{STRAPI_API_URL}/crawls/{item_id}",
                            headers={"Content-Type": "application/json"},
                            timeout=30.0,
                        )

            # 3. ìƒˆ ë°ì´í„° ì „ë¶€ ì €ì¥
            for result in results:
                await client.post(
                    f"{STRAPI_API_URL}/crawls",
                    headers={"Content-Type": "application/json"},
                    json={"data": result},
                    timeout=30.0,
                )

            print(f"âœ… Strapi ì €ì¥ ì™„ë£Œ: {len(results)} ê°œ")

        except Exception as e:
            print("âŒ Strapi ì €ì¥ ì˜¤ë¥˜:", e)


@app.post("/crawl")
async def crawl(request: Request, background_tasks: BackgroundTasks):
    body = await request.json()

    site = body.get("site", "gmarket")
    keyword = body.get("keyword")
    include = body.get("include", [])
    exclude = body.get("exclude", [])
    min_price = int(body.get("minPrice") or body.get("min_price") or 0)
    max_price = int(body.get("maxPrice") or body.get("max_price") or 999999999)

    async def crawl_and_save():
        result = []
        if site == "gmarket":
            result = await crawl_gmarket(keyword, include, exclude, min_price, max_price)
        elif site == "11st":
            result = await crawl_elevenst(keyword, include, exclude, min_price, max_price)
        else:
            print("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸:", site)
            return
        await save_to_strapi(result)

    # âœ… ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ì‹¤í–‰
    background_tasks.add_task(crawl_and_save)

    return {"success": True, "message": "í¬ë¡¤ë§ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."}


if __name__ == "__main__":
    uvicorn.run("server:app", host="127.0.0.1", port=8000, reload=True)
